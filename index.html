<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="SeTok: Towards Semantic-equivalent Tokenization for MLLM">
    <meta name="keywords" content="Multimodal Large Language Models">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SeTok</title>

    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.cdnfonts.com/css/caveat" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
    <link rel="icon" href="./static/images/logo.png">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
<!--    <link rel="stylesheet" href="bulma-carousel.min.css">-->
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <link rel="stylesheet" href="./static/css/index-gradio.css">
    <link rel="stylesheet" href="./static/css/live_theme.css">

    <style>
        @import url('https://fonts.cdnfonts.com/css/caveat');
    </style>

<!--    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>-->
    <script defer src="./static/js/fontawesome.all.min.js"></script>
<!--    <script src="./static/js/bulma-carousel.min.js"></script>-->
<!--    <script src="./static/js/bulma-slider.min.js"></script>-->
    <script src="./static/js/index.js"></script>

<!--    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"-->
<!--          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">-->
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- <h1 class="title is-1 publication-title" -->
                        <!-- style="display: flex;flex-direction: row;align-items: center;justify-content: center;margin-bottom: 5px;"><img -->
                            <!-- src="./static/images/logo.png" width="60" height="60" style="margin-right: 25px;margin-bottom: 6px;"></h1> -->
                    <h1 class="title is-2 publication-title">Towards Semantic Equivalence of Tokenization in Multimodal LLM</h1>
<!--                     <h1 class="title is-2 publication-title" style="margin-top: -25px">Understanding, Generating, Segmenting, Editing</h1> -->
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://chocowu.github.io/">Shengqiong Wu</a><sup>1,2</sup></span> &nbsp;
                        <span class="author-block">
                <a href="https://haofei.vip/">Hao Fei</a><sup>1,2</sup> </span> &nbsp;
                        <span class="author-block">
                <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>2</sup> </span> &nbsp;
                    <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xp_rICcAAAAJ&hl=zh-CN">Jiayi Ji</a><sup>1</sup> </span> &nbsp;
                    <span class="author-block">
              <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a><sup>2,3</sup></span> &nbsp;
                        <span class="author-block">
              <a href="https://www.chuatatseng.com/">Tat-Seng Chua</a><sup>1</sup></span> &nbsp;
                        <span class="author-block">
              <a href="https://yanshuicheng.info/">Shuicheng Yan</a><sup>2</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors" style="margin-top: 10px;">
                        <span class="author-block"><b style="color:#f68946; font-weight:normal">▶ </b><sup>1</sup>National University of Singapore</span> &nbsp;
                        <span class="author-block"><b
                                style="color:#008AD7; font-weight:normal">▶ </b><sup>2</sup>Skywork AI, Singapore</span> &nbsp;
                        <span class="author-block"><b
                                style="color:#00d754; font-weight:normal">▶ </b><sup>3</sup>Nanyang Technological University</span>
                    </div>

                    <!-- <div class="is-size-5 publication-authors">
                        <span class="author-block" style="font-size: 15px;">(<sup>*</sup>Correspondence)</span>
                    </div> -->

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                                <a href="xxx" class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                            </a>
                        </span>

                <!-- <span class="link-block">
                    <a href="xxx" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fa fa-laugh"></i>
                    </span>
                    <span>Demo</span>
                    </a>
                </span> -->
                
                <!-- Video Link. -->
                <!-- <span class="link-block">
                    <a href="xxx" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                    </a>
                </span> -->


                <!-- Code Link. -->
                <span class="link-block">
                    <a href="https://github.com/ChocoWu/SeTok"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>


                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
                <h2 class="title is-2">Abstraction</h2>
                <div class="content has-text-justified">
                    <p>
                        Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in processing vision-language tasks.
                        One of the crux of MLLMs lies in vision tokenization, which involves efficiently transforming input visual signals into feature representations that are most beneficial for LLMs.
                        However, existing vision tokenizers, essential for semantic alignment between vision and language, remain problematic.
                        Existing methods aggressively fragment visual input, corrupting the visual semantic integrity.
                        To address this, this paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (<b>SeTok</b>), which groups visual features into semantic units via a dynamic clustering algorithm, flexibly determining the number of tokens based on image complexity.
                        The resulting vision tokens effectively preserve semantic integrity and capture both low-frequency and high-frequency visual features.
                        The proposed MLLM (<b>Setokim</b>) equipped with SeTok significantly demonstrates superior performance across various tasks, as evidenced by our experimental results.
                    </p>
                </div>
                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/intro.jpeg" alt="Teaser" width="100%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: left;">
                            <font color="061E61">
                                <b>Figure 1:</b> Comparisons between existing MLLMs in tokenizing input image via (a) Patchifying image, (b) Codebook, and (c) Cluster Merger.
                                In (d), we show four language-driven vision tasks enhanced with semantic-equivalent vision tokens where regions with the same color denote a vision token.
                            </font>
                        </p>
                    </figcaption>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
                <h2 class="title is-2">Method</h2>
                <div class="content has-text-justified">
                    <p>
                        (a) A <b>Vision Cluster</b> is automatically grouping visual features from input images into dynamic number of visual clusters. (b) The <b>Vision Merger</b> is proposed to aggregate visual embeddings beyond merely using cluster centers as definitive vision tokens.
                        (b) The core backbone is a LLM. (c) The <b>Vision Decoder</b> to decode realistic images by taking the tokenized visual tokens as inputs. (d) The <b>Mask Decoder</b> is taking the vision tokens as input to decode the object mask.
                    </p>
                </div>
                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/framework.jpeg" alt="Teaser" width="100%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: left;">
                            <font color="061E61">
                                <b>Figure 2:</b> The overview of <b>SeTokim</b>.  
                                The visual embedding extracted from a vision encoder is tokenized into vision tokens by <b>SeTok</b>, combined with text tokens to be input LLM for vision-language understanding. 
                                The output vision embeddings from LLM are fed into a vision decoder and mask decoder to generate realistic images and semantic segmentation masks, respectively. 
                            
                            </font>
                        </p>
                    </figcaption>
                </div>
            </div>
        </div>


        <!-- Training -->
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">Training</h2>

                <div class="content has-text-justified">
                    <p>
                        We train the Setokim to endow it with robust vision understanding and task execution capabilities, with three distinct phases.
                    <ul>
                        <li>
                            <b>Step-I: Tokenizer Pretraining.</b>
                            This stage aims to truly endow the tokenizer with the capability of tokenizing the input vision into a semantic complement and independent tokens that can capture the low-frequency semantic features and high-frequency pixel features. 

                        </li>
                        <li>
                            <b>Step-II: Multimodal Pretraining.</b>
                            In this phase, we enhance the LLM to possess interleaved understanding and generation capabilities for vision-language tasks.
                            Thus, on the one hand, we adopt next-token prediction-based cross-entropy loss for the textual content generation.
                            Meanwhile, we employ embedding regression loss to train LLM to generate visual tokens, which are trained to reconstruct the features on the pre-trained vision tokenizer with a Mean Squared Error (MSE) loss. 
                        </li>
                        <li>
                            <b>Step-III: Instruction Tuning.</b>
                            We perform multimodal instruction tuning through fine-tuning LLM using a LoRA module with both public datasets covering fine-grained visual QA, image generation and editing, and text-rich grounded datasets.
                        </li>
                    </ul>
                    </p>
                </div>
                <br/>

            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">

        <div class="columns is-centered has-text-centered">
            <h2 class="title is-2">Qualitative Analysis</h2>
            <br>
        </div>

        <div class="columns is-centered">
            <div class="column is-full-width">
                <h4 class="title is-3">• Visual Understanding and Generation</h4>

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/visual-under.jpeg" alt="Teaser" width="95%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: left;">
                            <font color="061E61">
                                <b>Figure 3:</b> Qualitative results on image understanding and generation.
                                The <span style="color: rgb(39,190,110);">words</span> marked in green are key elements in questions and answers.
                            </font>
                        </p>
                    </figcaption>
                </div>
                <br/>

            </div>
        </div>

        <div class="columns is-centered">
            <div class="column is-full-width">
                <h4 class="title is-3">• Visual segmentation</h4>

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/segmentation.jpeg" alt="Teaser" width="95%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: center;">
                            <font color="061E61">
                                <b>Figure 4:</b> The visualizations for segmentation results compared with GLaMM and Ospery.
                            </font>
                        </p>
                    </figcaption>
                </div>
                <br/>

            </div>
        </div>


        <div class="columns is-centered">
            <div class="column is-full-width">
                <h4 class="title is-3">• Visual Editing</h4>

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/visual-editing.jpeg" alt="Teaser" width="95%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: left;">
                            <font color="061E61">
                                <b>Figure 5:</b> Qualitative comparison between MLLMs for the image editing. 
                                <b>Setokim</b> excels in adhering to instructions and preserving low-level image details.
                            </font>
                        </p>
                    </figcaption>
                </div>
                <br/>

            </div>
        </div>
        
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h4 class="title is-3">• Visual Tokens</h4>

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/cluster.jpeg" alt="Teaser" width="95%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: center;">
                            <font color="061E61">
                                <b>Figure 6:</b> The visualizations for visual tokens.
                            </font>
                        </p>
                    </figcaption>
                </div>
                <br/>

            </div>
        </div>
        

        <div class="columns is-centered">
            <div class="column is-full-width">
                <h4 class="title is-3">• Reconstructions</h4>

                <div class="content has-text-justified">
                    <img class="columns is-centered has-text-centered" src="./static/images/reconstruction.jpeg" alt="Teaser" width="95%"
                         style="margin:0 auto">
                    <br>
                    <figcaption>
                        <p style="text-align: center;">
                            <font color="061E61">
                                <b>Figure 7:</b> The image reconstruction results from visual tokens by the denoising U-Net.
                            </font>
                        </p>
                    </figcaption>
                </div>
                <br/>

            </div>
        </div>

    </div>
</section>



<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{wu2024towards,
  title={Towards Semantic Equivalence of Tokenization in Multimodal LLM},
  author={Wu, Shengqiong and Fei, Hao and Li, Xiangtai and Ji, Jiayi and Zhang, Hanwang and Chua, Tat-Seng and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2406.05127},
  year={2024}
}
</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p style="text-align: center;">
                        The webpage is built based on <a href="https://next-gpt.github.io/">NExT-GPT</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
